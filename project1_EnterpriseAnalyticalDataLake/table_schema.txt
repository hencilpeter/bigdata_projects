data_transformation
data_manipulation
==================
drop_exact_duplicate_records
drop_records_when_column_value
filter_record

add_column
rename_column
drop_column 
replace_column_value
cast_column_type (from to ) 


Transformation 
CapitalizeWordTransformation 
NumberTransformationRuleImpl.scala
DateTransformationRuleImpl.scala

a.Handlingduplicates
b.DealingwithNullvalues
c.HandlingMissingValues
d.ChangingDatatypes




steps :
1. develop a quick prototype 
2. make it production ready 
3. improve the performance 

/usr/bin/python3.6 validator_main.py 
/usr/bin/python3.6 validator_main.py 20230927
/usr/bin/python3.6 validator_main.py 20230928
/usr/bin/python3.6 validator_main.py 20230929
/usr/bin/python3.6 validator_main.py 20230930
----
1. schema names:
----------------
hp_config 
hp_stage
hp_core 	(for core tables and views. view prefix: v_ ) 
hp_audit


2. table names:
----------------
config tables (pipe delimiter)
*************
2.1 hp_config.staging_to_core
CREATE TABLE hp_config.staging_to_core(id int, source_application string, filename string,source_hdfs_path string, source_table string, target_hdfs_path string, target_table string, data_load_type string, should_add_partition int, is_active int, create_timestamp timestamp, created_by string, update_timestamp timestamp, updated_by string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' ;

2.2 hp_config.optimization_base
CREATE TABLE hp_config.optimization_base(id int, optimization_category string, optimization_sub_category string, optimization_name string,  parameter_format string, description string, is_active int,create_timestamp timestamp,created_by string, update_timestamp timestamp,updated_by string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';

2.3 hp_config.optimization_staging_to_core
CREATE TABLE IF NOT EXISTS hp_config.optimization_staging_to_core(id int,table_name string,optimization_category
 string,optimization_name string,parameter_value string,is_active int,create_timestamp timestamp,created_by  string,update_timestamp timestamp,updated_by string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' ;

data tables - staging layer 
***********
(i) hp_stage.account
CREATE EXTERNAL TABLE hp_stage.account(account_id string, district_id string, frequency string,date string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','  LINES TERMINATED BY '\n' PARTITIONED BY(as_of_date string) location '/user/itv007175/datalake/stage/account/';
(ii) hp_stage.card 


data tables - core layer 
***********
1.hp_core.account 
CREATE EXTERNAL TABLE IF NOT EXISTS hp_core.account(created_by string,create_timestamp timestamp ,updated_by string ,update_timestamp timestamp,account_id long ,district_id long,account_creation_date date,frequency string)STORED AS PARQUET PARTITIONED BY(as_of_date long) LOCATION '/user/itv007175/datalake/core/account';

2. client - new 
create_table("CREATE EXTERNAL TABLE hp_stage.client(client_id string,birth_number string,district_id string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','  LINES TERMINATED BY '\n' PARTITIONED BY(as_of_date string) location '/user/itv007175/datalake/stage/client/' TBLPROPERTIES ('skip.header.line.count'='1');")

CREATE EXTERNAL TABLE IF NOT EXISTS hp_core.client(created_by string,create_timestamp timestamp ,updated_by string ,update_timestamp timestamp,client_id long ,birth_number long,district_id long)STORED AS PARQUET PARTITIONED BY(as_of_date long) LOCATION '/user/itv007175/datalake/core/client';

3. disp 
create_table("CREATE EXTERNAL TABLE hp_stage.disp(disp_id string, client_id string ,account_id string,type string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','  LINES TERMINATED BY '\n' PARTITIONED BY(as_of_date string) location '/user/itv007175/datalake/stage/disp/' TBLPROPERTIES ('skip.header.line.count'='1');")

CREATE EXTERNAL TABLE IF NOT EXISTS hp_core.disp(created_by string,create_timestamp timestamp ,updated_by string ,update_timestamp timestamp,disp_id long, client_id long ,account_id long,type string)STORED AS PARQUET PARTITIONED BY(as_of_date long) LOCATION '/user/itv007175/datalake/core/disp';

4.loan
CREATE EXTERNAL TABLE IF NOT EXISTS hp_core.loan(created_by string,create_timestamp timestamp ,updated_by string ,update_timestamp timestamp,loan_id long, account_id long,loan_creation_date date,amount double,duration long,payments double,status string)STORED AS PARQUET PARTITIONED BY(as_of_date long) LOCATION '/user/itv007175/datalake/core/loan';
 
hadoop fs -ls /user/itv007175/datalake/core/*/*
hadoop fs -rm -r /user/itv007175/datalake/core/*/*

5. orders 
order_id	-> long 
account_id	-> long 
bank_to	-> string 
account_to	-> long 
amount	-> double 
k_symbol -> "K_symbol": "payment_category"
6. trans  -> transactions 
trans_id	-> long 
account_id	-> long 
date	-> yyyy-MM-dd
type -> string 	
operation -> string 	
amount	-> double 
balance	-> double 
k_symbol "K_symbol": "payment_category"	
bank	-> string 
account -> long



TO DO LSIT:
1. fix create_timestamp() in config tables 
2. staging layer - remove hardcoded values in main script 




========================================


table: config.optimizatation_landing_to_staging
       config.optimizatation_staging_to_core   
id, table_name, optimization_name, parameter_name, parameter_value, is_active, create_timestamp, update_timestamp, updated_by  


table: config.table_columns
id, table_name, column_name, is_primary_key, is_nullable , is_active, create_timestamp, created_by, last_update_timestamp, updated_by    



table: audit.landing_to_staging
			id, source_application,  filename_body, audit_category, status, audit_data, audit_date, audit_timestamp		
	   audit.staging_to_core
			id, source_application,  filename_body, audit_category, status, audit_data, audit_date, audit_timestamp			

table: data.
       curated layer:
	   additional columns: as_of_date, modified_by, modified_timestamp,  created_by, created_timestamp 
	   
	   
	   
table_creation:


-----basic tables:
config.staging_to_core
config.optimization
optimization.staging_to_core
audit.staging_to_core


 /user/itv007175/stage/
 /user/itv007175/core/ 
 
 
 ===============
 
 ---------------
live project tasks 
==================
[done]1. design all the tables (count : 4 )  - done 
[done]2. create tables  - done 
[done]3. fill the data - done 
4. create data tables 
[done]5. correct the staging code to copy the file 

6. spark application to read the configuraiton 
7. apply basic cleaning 
8. apply basic transformation 
9. write to the target table and add the new partition 

10. populate the audit info 

11. log the details 
---
1. account : "account_id";"district_id";"frequency";"date"
2. card: "card_id";"disp_id";"type";"issued"
3.
4.
---
transformations:
1. table: account , column : frequency, transformation: remove quotes 
2. table: account , column : frequency, transformation: initcap
3. table: account , column : date, transformation: 

current:
[done]i. remove quotes 
[done]ii. remove first line 
[done]iii. capitialize 
[done]iv. date format change 
[done]v. add prefix columns  
[done]i. hp_config.staging_to_core - change column values  (source_hdfs_path , target_hdfs_path )
[done]ii. hp_config.optimization_staging_to_core - add/modify values 
[done]iii. read hp_config.optimization_staging_to_core table  
[done]iv. dynamic cleansing 
v. dynamic transformation 
vi. add new tables 

transform_dateformat_yymmdd_to_yyyyMMdd
transform_rename_column
transform_initcap
transform_string_to_long
 

1. card 
   card_id-> long	
   disp_id ->long	
   type	 -> intcap 
[new]issued - timestamp(
2. client - used 
   client_id	 -> long
   birth_number	 -> long 
   district_id   -> long 
   
3. disp 
disp_id	 -> long 
client_id -> long 	
account_id-> long 	
type -> string 
4.loan 
loan_id -> long 
account_id	-> long 
date	->yyyy-MM-dd
amount	-> double 
duration -> long 	
payments -> double 	
status -> string 
5. orders 
order_id	-> long 
account_id	-> long 
bank_to	-> string 
account_to	-> long 
amount	-> double 
k_symbol -> "K_symbol": "payment_category"
6. trans  -> transactions 
trans_id	-> long 
account_id	-> long 
date	-> yyyy-MM-dd
type -> string 	
operation -> string 	
amount	-> double 
balance	-> double 
k_symbol "K_symbol": "payment_category"	
bank	-> string 
account -> long 

7. distict 
A1	-> district_id -> long 
A2	-> district_name -> string 
A3	-> region -> string 
A4	-> inhabitants_count -> long 
A5	->  inhabitants_count_lessthan_400 -> long 
A6	->  inhabitants_count_bet_500_1999 -> long 
A7	->  inhabitants_count_bet_2000_9999 -> long 
A8	->  inhabitants_count_abive_10000 -> long 
A9	-> cities_count -> long 
A10	-> urban_inhabitants_ratio -> double 
A11	-> average_salary -> double 
A12	-> unemployment_rate_1995 -> double 
A13	-> unemployment_rate_1996 -> double 
A14	-> entre_per_1000_inhabitants -> long 
A15	-> crimes_commited_1995 -> long 
A16 -> crimes_commited_1996 -> long 








           




/home/itv007175/data/landing/20230927/card.csv
/user/itv007175/datalake/stage 
/user/itv007175/datalake/core


subprocess.call(['hadoop fs -copyFromLocal /home/itv007175/data/landing/20230927/card.csv hdfs:///user/edwaeadt/app'], shell=True)

--staging script 
/home/itv007175/project_src/staging_data_processor   /usr/bin/python3.6 validator_main.py

--core layer script 
spark-submit core_data_processor_main.py 20230928




 |hdfs://m01.itversity.com:9000/user/itv007175/datalake/stage/account|       |
|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                 


/user/itv007175/datalake/stage/account/20230927
[itv007175@g02 ~]$ hadoop fs -ls -R /user/itv007175/datalake/stage/hadoop fs -cat /user/itv007175/datalake/stage/account/20230927
hadoop fs -cat /user/itv007175/datalake/stage/hadoop fs -cat /user/itv007175/datalake/stage/account/20230927


  
hdfs://m01.itversity.com:9000/user/itv007175/datalake/stage/account/20230927/account.csv
[itv007175@g02 ~]$ hadoop fs -head 

/home/itv007175/project_src/staging_data_processor
[itv007175@g02 staging_data_processor]$ 

AttributeError: 'NoneType' object has no attribute 'setCallSite'
[itv007175@g02 core_data_processor]$ 