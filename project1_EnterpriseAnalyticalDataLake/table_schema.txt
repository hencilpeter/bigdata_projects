data_transformation
data_manipulation
==================
drop_exact_duplicate_records
drop_records_when_column_value
filter_record

add_column
rename_column
drop_column 
replace_column_value
cast_column_type (from to ) 


Transformation 
CapitalizeWordTransformation 
NumberTransformationRuleImpl.scala
DateTransformationRuleImpl.scala

a.Handlingduplicates
b.DealingwithNullvalues
c.HandlingMissingValues
d.ChangingDatatypes




steps :
1. develop a quick prototype 
2. make it production ready 
3. improve the performance 

/usr/bin/python3.6 validator_main.py 
/usr/bin/python3.6 validator_main.py 20230927
/usr/bin/python3.6 validator_main.py 20230928
/usr/bin/python3.6 validator_main.py 20230929
/usr/bin/python3.6 validator_main.py 20230930
----
1. schema names:
----------------
hp_config 
hp_stage
hp_core 	(for core tables and views. view prefix: v_ ) 
hp_audit


2. table names:
----------------
config tables (pipe delimiter)
*************
2.1 hp_config.staging_to_core
CREATE TABLE hp_config.staging_to_core(id int, source_application string, filename string,source_hdfs_path string, source_table string, target_hdfs_path string, target_table string, data_load_type string, should_add_partition int, is_active int, create_timestamp timestamp, created_by string, update_timestamp timestamp, updated_by string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' ;

2.2 hp_config.optimization_base
CREATE TABLE hp_config.optimization_base(id int, optimization_category string, optimization_sub_category string, optimization_name string,  parameter_format string, description string, is_active int,create_timestamp timestamp,created_by string, update_timestamp timestamp,updated_by string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';

2.3 hp_config.optimization_staging_to_core
CREATE TABLE hp_config.optimizatation_staging_to_core(id int,table_name string,optimization_name string,parameter_value string,is_active int,create_timestamp timestamp,created_by  string,update_timestamp timestamp,updated_by string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' ;

data tables - staging layer 
***********
(i) hp_stage.account
CREATE EXTERNAL TABLE hp_stage.account(account_id string, district_id string, frequency string,date string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','  LINES TERMINATED BY '\n' PARTITIONED BY(as_of_date string) location '/user/itv007175/datalake/stage/account/';
(ii) hp_stage.card 


data tables - core layer 
***********
1.hp_core.account 
CREATE EXTERNAL TABLE IF NOT EXISTS hp_core.account(created_by string,create_timestamp timestamp ,updated_by string ,update_timestamp timestamp,account_id long ,district_id long,account_creation_date date,frequency string)STORED AS PARQUET PARTITIONED BY(as_of_date long) LOCATION '/user/itv007175/datalake/core/account';



TO DO LSIT:
1. fix create_timestamp() in config tables 
2. staging layer - remove hardcoded values in main script 




========================================


table: config.optimizatation_landing_to_staging
       config.optimizatation_staging_to_core   
id, table_name, optimization_name, parameter_name, parameter_value, is_active, create_timestamp, update_timestamp, updated_by  


table: config.table_columns
id, table_name, column_name, is_primary_key, is_nullable , is_active, create_timestamp, created_by, last_update_timestamp, updated_by    



table: audit.landing_to_staging
			id, source_application,  filename_body, audit_category, status, audit_data, audit_date, audit_timestamp		
	   audit.staging_to_core
			id, source_application,  filename_body, audit_category, status, audit_data, audit_date, audit_timestamp			

table: data.
       curated layer:
	   additional columns: as_of_date, modified_by, modified_timestamp,  created_by, created_timestamp 
	   
	   
	   
table_creation:


-----basic tables:
config.staging_to_core
config.optimization
optimization.staging_to_core
audit.staging_to_core


 /user/itv007175/stage/
 /user/itv007175/core/ 
 
 
 ===============
 
 ---------------
live project tasks 
==================
[done]1. design all the tables (count : 4 )  - done 
[done]2. create tables  - done 
[done]3. fill the data - done 
4. create data tables 
[done]5. correct the staging code to copy the file 

6. spark application to read the configuraiton 
7. apply basic cleaning 
8. apply basic transformation 
9. write to the target table and add the new partition 

10. populate the audit info 

11. log the details 
---
1. account : "account_id";"district_id";"frequency";"date"
2. card: "card_id";"disp_id";"type";"issued"
3.
4.
---
transformations:
1. table: account , column : frequency, transformation: remove quotes 
2. table: account , column : frequency, transformation: initcap
3. table: account , column : date, transformation: 

current:
i. remove quotes 
ii. remove first line 
iii. capitialize 
iv. date format change 
v. add prefix columns  



https://stackoverflow.com/questions/76361665/pyspark-convert-string-to-date-type
data_sdf. \
    withColumn('new_str_dt', 
               func.when(func.substring('str_dt', 8, 2).cast('int').between(0, 25), 
                         func.concat(func.substring('str_dt', 1, 7), func.lit('20'), func.substring('str_dt', 8, 2))
                         ).
               otherwise(func.concat(func.substring('str_dt', 1, 7), func.lit('19'), func.substring('str_dt', 8, 2)))
               ). \
    withColumn('dt', func.to_date('new_str_dt', 'dd-MMM-yyyy')). \
    show()




/home/itv007175/data/landing/20230927/card.csv
/user/itv007175/datalake/stage 
/user/itv007175/datalake/core


subprocess.call(['hadoop fs -copyFromLocal /home/itv007175/data/landing/20230927/card.csv hdfs:///user/edwaeadt/app'], shell=True)

--staging script 
/home/itv007175/project_src/staging_data_processor   /usr/bin/python3.6 validator_main.py

--core layer script 
spark-submit core_data_processor_main.py 20230928




 |hdfs://m01.itversity.com:9000/user/itv007175/datalake/stage/account|       |
|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                 


/user/itv007175/datalake/stage/account/20230927
[itv007175@g02 ~]$ hadoop fs -ls -R /user/itv007175/datalake/stage/hadoop fs -cat /user/itv007175/datalake/stage/account/20230927
hadoop fs -cat /user/itv007175/datalake/stage/hadoop fs -cat /user/itv007175/datalake/stage/account/20230927


  
hdfs://m01.itversity.com:9000/user/itv007175/datalake/stage/account/20230927/account.csv
[itv007175@g02 ~]$ hadoop fs -head 

/home/itv007175/project_src/staging_data_processor
[itv007175@g02 staging_data_processor]$ 